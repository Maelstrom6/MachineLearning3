install.packages("caTools")
q()
setwd("~/PycharmProjects/MachineLearning2/ReinforcementLearning/UpperConfidenceBound")
dataset = read.csv("Ads_CTR_Optimisaion.csv")
N = 10000  # number of rounds to play
d = 10  # number of different ads
n_i = c(0) * d  # number of times ad i was selected up to round n
r_i = c(0) * d  # sum of rewards of ad i up to round n
dataset = read.csv("Ads_CTR_Optimisation.csv")
n_i = vector(0) * d
n_i = list(0) * d
for n in 1:N {
}
for (n in 1:N) {
d = d + 1
}
d = 10
n_i = integer(d)
r_i = integer(d)
sqrt(3)
log(3)
print(n_i)
print(sum(r_i))
for (n in 1:N) {
max_index = 0
max_bound = 0
for (i in 1:d) {
if (n_i[i] == 0){
r_bar_i = 0.5
delta_i = 10000
}else{
r_bar_i = (r_i[i] / n_i[i])  # the sample proportion of rewards
delta_i = (sqrt(3 / 2 * log(n) / n_i[i]))  # the sample variance of rewards
}
upper_bound = r_bar_i + delta_i
if (max_bound < upper_bound){
max_bound = upper_bound
max_index = i
}
}
result = dataset[n, max_index]
n_i[max_index] = n_i[max_index] + 1
r_i[max_index] = r_i[max_index] + result
}
print(n_i)
print(r_i)
print(sum(r_i))
N = 10000  # number of rounds to play
d = 10  # number of different ads
n_i = integer(d)  # number of times ad i was selected up to round n
r_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_bound = 0
for (i in 1:d) {
if (n_i[i] == 0){
r_bar_i = 0.5
delta_i = 10000
}else{
r_bar_i = (r_i[i] / n_i[i])  # the sample proportion of rewards
delta_i = (sqrt(3 / 2 * log(n) / n_i[i]))  # the sample variance of rewards
}
upper_bound = r_bar_i + delta_i
if (max_bound < upper_bound){
max_bound = upper_bound
max_index = i
}
}
result = dataset[n, max_index]
n_i[max_index] = n_i[max_index] + 1
r_i[max_index] = r_i[max_index] + result
}
N = 10000  # number of rounds to play
d = 10  # number of different ads
n_i = integer(d)  # number of times ad i was selected up to round n
r_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_bound = 0
for (i in 1:d) {
if (n_i[i] == 0){
r_bar_i = 0.5
delta_i = 10000
}else{
r_bar_i = (r_i[i] / n_i[i])  # the sample proportion of rewards
delta_i = (sqrt(3 / 2 * log(n) / n_i[i]))  # the sample variance of rewards
}
upper_bound = r_bar_i + delta_i
if (max_bound < upper_bound){
max_bound = upper_bound
max_index = i
}
}
result = dataset[n, max_index]
n_i[max_index] = n_i[max_index] + 1
r_i[max_index] = r_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r_i))
hist(ads_selected,
col = "blue",
main = paste("Histogram of ad selections"),
xlab = "Add number",
ylab = "Number of selections")
plot(1:d, r_i/n_i)
barplot(1:d, r_i/n_i)
setwd("~/PycharmProjects/MachineLearning2/ReinforcementLearning/ThompsonSampling")
N = 10000  # number of rounds to play
d = 10  # number of different ads
r0_i = integer(d)  # sum of punishments of ad i up to round n
r1_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_random = 0
for (i in 1:d) {
random_beta = beta(r1_i[i] + 1, r0_i[i] + 1)
if (max_random < random_beta){
max_random = random_beta
max_index = i
}
}
result = dataset[n, max_index]
r0_i[max_index] = r0_i[max_index] + 1 - result
r1_i[max_index] = r1_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r1_i))
hist(ads_selected,
col = "blue",
main = paste("Histogram of ad selections"),
xlab = "Add number",
ylab = "Number of selections")
N = 10000  # number of rounds to play
d = 10  # number of different ads
r0_i = integer(d)  # sum of punishments of ad i up to round n
r1_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_random = 0
for (i in 1:d) {
random_beta = beta(r1_i[i] + 1, r0_i[i] + 1)
if (max_random < random_beta){
max_random = random_beta
max_index = i
}
}
result = dataset[n, max_index]
r0_i[max_index] = r0_i[max_index] + 1 - result
r1_i[max_index] = r1_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r1_i))
N = 10000  # number of rounds to play
d = 10  # number of different ads
r0_i = integer(d)  # sum of punishments of ad i up to round n
r1_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_random = 0
for (i in 1:d) {
random_beta = beta(r1_i[i] + 1, r0_i[i] + 1)
if (max_random < random_beta){
max_random = random_beta
max_index = i
}
}
result = dataset[n, max_index]
r0_i[max_index] = r0_i[max_index] + 1 - result
r1_i[max_index] = r1_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r1_i))
dataset = read.csv("Ads_CTR_Optimisation.csv")
N = 10000  # number of rounds to play
d = 10  # number of different ads
r0_i = integer(d)  # sum of punishments of ad i up to round n
r1_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_random = 0
for (i in 1:d) {
random_beta = beta(r1_i[i] + 1, r0_i[i] + 1)
if (max_random < random_beta){
max_random = random_beta
max_index = i
}
}
result = dataset[n, max_index]
r0_i[max_index] = r0_i[max_index] + 1 - result
r1_i[max_index] = r1_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r1_i))
beta(5, 5)
beta(5, 5)
rbeta( 5, 5)
rbeta(n=1, shape1 = 5, shape2 = 5)
rbeta(n=1, shape1 = 5, shape2 = 5)
N = 10000  # number of rounds to play
d = 10  # number of different ads
r0_i = integer(d)  # sum of punishments of ad i up to round n
r1_i = integer(d)  # sum of rewards of ad i up to round n
ads_selected= integer(0)
for (n in 1:N) {
max_index = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = r1_i[i] + 1,
shape2 = r0_i[i] + 1)
if (max_random < random_beta){
max_random = random_beta
max_index = i
}
}
result = dataset[n, max_index]
r0_i[max_index] = r0_i[max_index] + 1 - result
r1_i[max_index] = r1_i[max_index] + result
ads_selected = append(ads_selected, max_index)
}
print(sum(r1_i))
hist(ads_selected,
col = "blue",
main = paste("Histogram of ad selections"),
xlab = "Add number",
ylab = "Number of selections")
